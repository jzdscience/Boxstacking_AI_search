{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "from typing import List, Tuple, Dict, Any, Callable, Tuple\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import pysnooper\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper function to work on 2D tuple\n",
    "## originally work on Numpy array, but it is not hashable ... \n",
    "## Then I think directly work on 2d tuple may be better....\n",
    "\n",
    "def slice_tuple(rowstart, rowend, colstart, colend, tuple_input):\n",
    "    # just slicing tuple like numpy array[rowstart: rowend, colstart: colend]\n",
    "    list_1 = []\n",
    "    for i in range(rowstart, rowend):\n",
    "        list_2 = []\n",
    "        for j in range(colstart, colend):\n",
    "            ele = tuple_input[i][j]\n",
    "            list_2.append(ele)\n",
    "        list_1.append(tuple(list_2))\n",
    "    newtuple = tuple(list_1)\n",
    "    return newtuple\n",
    "\n",
    "def max_2d(tuple_2d):\n",
    "    # max value in 2d tuple\n",
    "    return max([max(x) for x in tuple_2d])\n",
    "\n",
    "\n",
    "def sum_2d(tuple_2d):\n",
    "    # sum of value in 2d tuple\n",
    "    return sum([sum(x) for x in tuple_2d])\n",
    "\n",
    "def set_tuple(rowstart, rowend, colstart, colend, old_tuple, value):\n",
    "    # replace value in tuple slicing\n",
    "    list_1 = [list(x) for x in old_tuple]\n",
    "    for i in range(rowstart, rowend):\n",
    "        for j in range(colstart, colend):\n",
    "            list_1[i][j] = value\n",
    "\n",
    "    return tuple(tuple(x) for x in list_1)\n",
    "\n",
    "def find_left(atuple):\n",
    "    # find left edge of box stack block within a row\n",
    "    list_of_index =[]\n",
    "    for i in range(len(atuple)):\n",
    "        if atuple[i] == 1:\n",
    "            list_of_index.append(i)\n",
    "    return min(list_of_index)\n",
    "\n",
    "def find_right(atuple):\n",
    "     # find right edge of box stack block within a row\n",
    "    list_of_index =[]\n",
    "    for i in range(len(atuple)):\n",
    "        if atuple[i] == 1:\n",
    "            list_of_index.append(i)\n",
    "    return max(list_of_index)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# An abstract class representing a Markov Decision Process (MDP).\n",
    "class MDP:\n",
    "    # Return the start state.\n",
    "    def startState(self) -> Tuple: raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    # Return set of actions possible from |state|.\n",
    "    def actions(\n",
    "        self, state: Tuple) -> List[Any]: raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    # Return a list of (newState, prob, reward) tuples corresponding to edges\n",
    "    # coming out of |state|.\n",
    "    # Mapping to notation from class:\n",
    "    #   state = s, action = a, newState = s', prob = T(s, a, s'), reward = Reward(s, a, s')\n",
    "    # If IsEnd(state), return the empty list.\n",
    "    def succAndProbReward(\n",
    "        self, state: Tuple, action: Any) -> List[Tuple]: raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    def discount(self): raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    # Compute set of states reachable from startState.  Helper function for\n",
    "    # MDPAlgorithms to know which states to compute values and policies for.\n",
    "    # This function sets |self.states| to be the set of all states.\n",
    "    def computeStates(self):\n",
    "        raise NotImplementedError(\"Override me\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BoxMDP(MDP):\n",
    "    #                          \n",
    "    def __init__(self, boxList, space_width, space_height):\n",
    "\n",
    "        # box is (height, width)!!!!!!!!!!!\n",
    "        self.boxList = tuple(boxList)\n",
    "        self.space_width = space_width\n",
    "        self.space_height = space_height\n",
    "        self.space = tuple([tuple([0 for i in range(self.space_width)]) for j in range(self.space_height)])\n",
    "\n",
    "    # Return the start state.\n",
    "    # first element to record the space, second element is box list, third element is box picked\n",
    "\n",
    "    def startState(self):\n",
    "        return (self.space, self.boxList, None)\n",
    "\n",
    "    def actions(self, state) :\n",
    "        # if box is not picked: action is like 'pick0' , means we pick #0 box in the box tuple\n",
    "        if len(state[1]) >0 and state[2] is None:\n",
    "            actions = ['pick'+str(i) for i in range(len(state[1]))]\n",
    "        # if box is picked: action is like 'drop0', means we drop the box on hand to location 0 (x-axis)\n",
    "        elif state[2] is not None:\n",
    "            avail_loc = list(range(self.space_width - state[2][1] + 1))\n",
    "            actions = ['drop'+str(loc) for loc in avail_loc]\n",
    "        # if no box on hand, no box left in tuple, we have to end\n",
    "        elif len(state[1]) == 0 and state[2] is None:\n",
    "            actions = ['end']\n",
    "        return actions\n",
    "\n",
    "    def isEnd(self, state):\n",
    "        ## if fallen, or the list is depleted & no box on hand, then it is end\n",
    "        if (state[0] is None) or (len(state[1]) == 0 and state[2] is None ):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    ## occupy the space\n",
    "    def drop_box(self, space, box, position):\n",
    "\n",
    "        space = deepcopy(space)\n",
    "\n",
    "        # drop box\n",
    "        fall = False\n",
    "        index_y = 0\n",
    "        index_x = position\n",
    "        \n",
    "        try:\n",
    "            maxdd = max_2d(slice_tuple(index_y, index_y + box[0] , index_x, index_x+ box[1], space)  ) \n",
    "        except:\n",
    "            print(index_y, index_y + box[0] , index_x, index_x+ box[1], space)\n",
    "            \n",
    "        # if space has been occupied then we add index by 1\n",
    "        while max_2d(slice_tuple(index_y, index_y + box[0] , index_x, index_x+ box[1], space)  ) >0:\n",
    "            index_y = index_y +1\n",
    "\n",
    "        space = set_tuple(index_y,index_y + box[0] , index_x, index_x+ box[1] , space, 1)\n",
    "\n",
    "        ## fall if gravity center go out\n",
    "        ### get the y below the new box\n",
    "        if index_y >= 1:\n",
    "            #if not on ground, calculate the gravity center\n",
    "            below_index_y = index_y -1\n",
    "            center_of_gravity = index_x + (box[1])/2\n",
    "            # check whether two side of gravity center both have suport \n",
    "            if sum_2d( slice_tuple(below_index_y, below_index_y+1,  index_x , math.ceil(center_of_gravity), space) ) == 0 or \\\n",
    "                sum_2d( slice_tuple(below_index_y, below_index_y+1, math.ceil(center_of_gravity)-1, index_x + (box[1]) ,space) ) == 0:\n",
    "            # if fall set space as empty\n",
    "                fall = True\n",
    "        \n",
    "        return space, fall\n",
    "\n",
    "\n",
    "    def calculate_gap(self, space):\n",
    "        # gaps on horizontal space\n",
    "        y_index = 0\n",
    "        total_hor_gap = 0\n",
    "        ## while y is small than row number\n",
    "        # print(space)\n",
    "        while y_index < len(space):\n",
    "            row_gap = 0\n",
    "\n",
    "            if sum(space[y_index]) >0:\n",
    "            # find the leftmost block and right most block\n",
    "                left = find_left(space[y_index])\n",
    "                right = find_right( space[y_index])\n",
    "                \n",
    "                row_gap = (right -left+1) - sum(space[y_index])\n",
    "                # print(left, right, row_gap)\n",
    "            y_index += 1\n",
    "            total_hor_gap += row_gap\n",
    "        return total_hor_gap\n",
    "\n",
    "    def calculate_unstability(self, space):\n",
    "        space_array = np.asarray(space)\n",
    "        # gaps on horizontal space\n",
    "        x_index = 0\n",
    "        total_vert_gap = 0\n",
    "\n",
    "        if space is None :\n",
    "            print('end state already')\n",
    "            return \n",
    "\n",
    "        while x_index < len(space_array[0]):\n",
    "            col_gap = 0\n",
    "            if sum(space_array[:,x_index]) >0:\n",
    "\n",
    "                top = 0\n",
    "                bottom = max(np.where(space_array[:,x_index] == 1)[0])\n",
    "                col_gap = bottom -top - (sum(space_array[:,x_index])-1)\n",
    "\n",
    "            x_index += 1\n",
    "            total_vert_gap += col_gap\n",
    "        return total_vert_gap\n",
    "\n",
    "    def reward(self, space):\n",
    "        '''\n",
    "        This reward function is very arbituary, subject to change\n",
    "\n",
    "        5*box_count -(self.calculate_unstability(space) + self.calculate_gap(space)) \n",
    "\n",
    "        '''\n",
    "        box_count = len(self.boxList)\n",
    "        # arbitually using 5* box count minus sum of unstability and gap\n",
    "#         return 5*box_count -(self.calculate_unstability(space) + self.calculate_gap(space))\n",
    "        return -(self.calculate_unstability(space) + self.calculate_gap(space))\n",
    "\n",
    "\n",
    "    def reward_2(self, space):\n",
    "        '''\n",
    "        This reward function is very arbituary, subject to change\n",
    "\n",
    "        5*box_count -(self.calculate_unstability(space) + self.calculate_gap(space)) \n",
    "\n",
    "        '''\n",
    "        occupied_pixel = sum([sum(row) for row in space])\n",
    "        # arbitually using 5* box count minus sum of unstability and gap\n",
    "#         return 5*box_count -(self.calculate_unstability(space) + self.calculate_gap(space))\n",
    "        return -(self.calculate_unstability(space) + self.calculate_gap(space))/occupied_pixel\n",
    "\n",
    "\n",
    "    # Given a |state| and |action|, return a list of (newState, prob, reward) tuples\n",
    "    # corresponding to the states reachable from |state| when taking |action|.\n",
    "    # A few reminders:\n",
    "    # * Indicate a terminal state (after quitting, busting, or running out of cards)\n",
    "    #   by setting the deck to None.\n",
    "    # * If |state| is an end state, you should return an empty list [].\n",
    "    # * When the probability is 0 for a transition to a particular new state,\n",
    "    #   don't include that state in the list returned by succAndProbReward.\n",
    "    def succAndProbReward(self, state: Tuple, action: str) -> List[Tuple]:\n",
    "        \n",
    "        if self.isEnd(state):\n",
    "            return []\n",
    "\n",
    "        state_list_to_return = []\n",
    "\n",
    "        if 'pick' in action and len(state[1]) >0:\n",
    "            ## randomly pick a box, prob is 1/len\n",
    "            prob = 1\n",
    "\n",
    "            ## add box ,newbox list to new state. Space does not change. Reward is 0\n",
    "            # for box in state[1]:\n",
    "\n",
    "            box_list = list(state[1])\n",
    "\n",
    "            box_index = int(action[-1])\n",
    "            box = box_list[box_index]\n",
    "\n",
    "            _ = box_list.remove(box)\n",
    "            new_box_tuple = tuple(box_list)\n",
    "\n",
    "            state_list_to_return.append( ((state[0], new_box_tuple, box), prob, 0))\n",
    "\n",
    "\n",
    "        if 'drop' in action and state[2] is not None:  # state[2] is box\n",
    "            \n",
    "            # we can drop to any x-position (use left of box as the marker of box position)\n",
    "            prob = 1\n",
    "\n",
    "            position = int(action[-1])\n",
    "\n",
    "            # for position in list(range(0, self.space_width - state[2][1] + 1)):\n",
    "                \n",
    "            # drop box state[0] is space; state[2] is box\n",
    "            new_space, fall = self.drop_box(state[0], state[2], position)\n",
    "\n",
    "            if fall is True:\n",
    "                # if fall, set the space to None\n",
    "                state_list_to_return.append(\n",
    "                    ( (None, state[1], None), prob, -999))\n",
    "            # if there is still box left to drop,\n",
    "            elif len(state[1]) > 0:\n",
    "                state_list_to_return.append(\n",
    "                    ((new_space, state[1], None), prob, 0))\n",
    "            # if there is not fall and no box left ,calculate the reward\n",
    "            else:\n",
    "                return [((new_space, state[1], None), 1, self.reward(new_space))]\n",
    "        \n",
    "        if 'end' in action:\n",
    "\n",
    "            return [((new_space, (), None), 1, self.reward(new_space))]\n",
    "\n",
    "\n",
    "        return state_list_to_return\n",
    "\n",
    "        # END_YOUR_CODE\n",
    "\n",
    "    def discount(self):\n",
    "        return 1\n",
    "\n",
    "    # Compute set of states reachable from startState.  Helper function for\n",
    "    # MDPAlgorithms to know which states to compute values and policies for.\n",
    "    # This function sets |self.states| to be the set of all states.\n",
    "\n",
    "    def computeStates(self):\n",
    "        self.states = set()\n",
    "        queue = []\n",
    "        self.states.add(self.startState())\n",
    "        queue.append(self.startState())\n",
    "        while len(queue) > 0:\n",
    "            state = queue.pop()\n",
    "            for action in self.actions(state):\n",
    "                for newState, prob, reward in self.succAndProbReward(state, action):\n",
    "                    if newState not in self.states:\n",
    "                        self.states.add(newState)\n",
    "                        queue.append(newState)\n",
    "        print(\"%d states\" % len(self.states))\n",
    "#         print(self.states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "############################################################\n",
    "\n",
    "# Abstract class: an RLAlgorithm performs reinforcement learning.  All it needs\n",
    "# to know is the set of available actions to take.  The simulator (see\n",
    "# simulate()) will call getAction() to get an action, perform the action, and\n",
    "# then provide feedback (via incorporateFeedback()) to the RL algorithm, so it can adjust\n",
    "# its parameters.\n",
    "class RLAlgorithm:\n",
    "    # Your algorithm will be asked to produce an action given a state.\n",
    "    def getAction(\n",
    "        self, state: Tuple) -> Any: raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    # We will call this function when simulating an MDP, and you should update\n",
    "    # parameters.\n",
    "    # If |state| is a terminal state, this function will be called with (s, a,\n",
    "    # 0, None). When this function is called, it indicates that taking action\n",
    "    # |action| in state |state| resulted in reward |reward| and a transition to state\n",
    "    # |newState|.\n",
    "    def incorporateFeedback(self, state: Tuple, action: Any, reward: int,\n",
    "                            newState: Tuple): raise NotImplementedError(\"Override me\")\n",
    "\n",
    "\n",
    "\n",
    "class QLearningAlgorithm(RLAlgorithm):\n",
    "    def __init__(self, actions: Callable, discount: float, featureExtractor: Callable, explorationProb=0.2):\n",
    "        self.actions = actions\n",
    "        self.discount = discount\n",
    "        self.featureExtractor = featureExtractor\n",
    "        self.explorationProb = explorationProb\n",
    "        self.weights = defaultdict(float)\n",
    "        self.numIters = 0\n",
    "\n",
    "    # Return the Q function associated with the weights and features\n",
    "    def getQ(self, state: Tuple, action: Any) -> float:\n",
    "        score = 0\n",
    "        for f, v in self.featureExtractor(state, action):\n",
    "            score += self.weights[f] * v\n",
    "        return score\n",
    "\n",
    "    # This algorithm will produce an action given a state.\n",
    "    # Here we use the epsilon-greedy algorithm: with probability\n",
    "    # |explorationProb|, take a random action.\n",
    "    def getAction(self, state: Tuple) -> Any:\n",
    "        self.numIters += 1\n",
    "        if random.random() < self.explorationProb:\n",
    "            return random.choice(self.actions(state))\n",
    "        else:\n",
    "            # print('second route of getting action')\n",
    "            return max((self.getQ(state, action), action) for action in self.actions(state))[1]\n",
    "\n",
    "    # Call this function to get the step size to update the weights.\n",
    "    def getStepSize(self) -> float:\n",
    "        return 1.0 / math.sqrt(self.numIters)\n",
    "\n",
    "    # We will call this function with (s, a, r, s'), which you should use to update |weights|.\n",
    "    # Note that if s is a terminal state, then s' will be None.  Remember to check for this.\n",
    "    # You should update the weights using self.getStepSize(); use\n",
    "    # self.getQ() to compute the current estimate of the parameters.\n",
    "    def incorporateFeedback(self, state: Tuple, action: Any, reward: int, newState: Tuple) -> None:\n",
    "        # BEGIN_YOUR_CODE (our solution is 9 lines of code, but don't worry if you deviate from this)\n",
    "        Q_opt_s_a_prime = 0.0\n",
    "\n",
    "        if newState is None:\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            # print(f'newState is {newState}')\n",
    "\n",
    "            # print(f'(self.getQ(newState, action), action) is {[ (self.getQ(newState, action), action) for action in self.actions(newState) ]}')\n",
    "\n",
    "            step = self.getStepSize()\n",
    "                \n",
    "            Q_opt_s_a_prime = max((self.getQ(newState, action), action)\n",
    "                                  for action in self.actions(newState))[0]\n",
    "\n",
    "            for item in self.featureExtractor(state, action):\n",
    "                key, value = item\n",
    "\n",
    "                self.weights[key] -= step*(self.getQ(key, action) -\n",
    "                                           (reward + self.discount*Q_opt_s_a_prime))*value\n",
    "\n",
    "        # END_YOUR_CODE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a single-element list containing a binary (indicator) feature\n",
    "# for the existence of the (state, action) pair.  Provides no generalization.\n",
    "\n",
    "\n",
    "def identityFeatureExtractor(state: Tuple, action: Any) -> List[Tuple[Tuple, int]]:\n",
    "    featureKey = (state, action)\n",
    "    featureValue = 1\n",
    "    return [(featureKey, featureValue)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NewFeatureExtractor(state: Tuple, action: str) -> List[tuple]:\n",
    "    space, box_tuple, box_on_hand = state\n",
    "\n",
    "    # BEGIN_YOUR_CODE (our solution is 7 lines of code, but don't worry if you deviate from this)\n",
    "    feature_list = []\n",
    "\n",
    "    feature_list.append(\n",
    "        (\n",
    "            ('total', total, action),\n",
    "            1)\n",
    "    )\n",
    "    \n",
    "    space \n",
    "\n",
    "    if counts is not None:\n",
    "        # print('bitmask', tuple([1 if counts[i] != 0 else 0 for i in range(len(counts)) ]))\n",
    "        feature_tuple = (('bitmask',\n",
    "                          tuple(\n",
    "                              [1 if counts[i] != 0 else 0 for i in range(len(counts))]),\n",
    "                          action), 1)\n",
    "        feature_list.append(feature_tuple)\n",
    "\n",
    "    if counts is not None:\n",
    "        for i in range(len(counts)):\n",
    "            feature_tuple = ((i, counts[i], action), 1)\n",
    "            feature_list.append(feature_tuple)\n",
    "\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform |numTrials| of the following:\n",
    "# On each trial, take the MDP |mdp| and an RLAlgorithm |rl| and simulates the\n",
    "# RL algorithm according to the dynamics of the MDP.\n",
    "# Each trial will run for at most |maxIterations|.\n",
    "# Return the list of rewards that we get for each trial.\n",
    "\n",
    "\n",
    "def simulate(mdp: MDP, rl: RLAlgorithm, numTrials=10, maxIterations=1000, verbose=False,\n",
    "             sort=False):\n",
    "    # Return i in [0, ..., len(probs)-1] with probability probs[i].\n",
    "    def sample(probs):\n",
    "        target = random.random()\n",
    "        accum = 0\n",
    "        for i, prob in enumerate(probs):\n",
    "            accum += prob\n",
    "            if accum >= target:\n",
    "                return i\n",
    "        raise Exception(\"Invalid probs: %s\" % probs)\n",
    "\n",
    "    totalRewards = []  # The rewards we get on each trial\n",
    "    for trial in range(numTrials):\n",
    "        state = mdp.startState()\n",
    "        sequence = [state]\n",
    "        totalDiscount = 1\n",
    "        totalReward = 0\n",
    "        # loop through iteration\n",
    "        for _ in range(maxIterations):\n",
    "            # get one action\n",
    "            action = rl.getAction(state)\n",
    "            # return transitions -- (new state,prob, reward)\n",
    "            transitions = mdp.succAndProbReward(state, action)\n",
    "            # print(f'transition table {transitions}')\n",
    "            if sort:\n",
    "                transitions = sorted(transitions)\n",
    "\n",
    "            # if no tranition available, break\n",
    "            if len(transitions) == 0:\n",
    "                # print('calling from rl.incorporateFeedback(state, action, 0, None)')\n",
    "                rl.incorporateFeedback(state, action, 0, None)\n",
    "                break\n",
    "\n",
    "            # Choose a random transition\n",
    "            i = sample([prob for newState, prob, reward in transitions])\n",
    "\n",
    "            # print(f'transitions[i] is {transitions[i]}')\n",
    "\n",
    "            newState, prob, reward = transitions[i]\n",
    "\n",
    "            # print(f'newState {newState} prob {prob}  reward {reward}')\n",
    "\n",
    "\n",
    "            sequence.append(action)\n",
    "            sequence.append(reward)\n",
    "            sequence.append(newState)\n",
    "\n",
    "            # incorcoprate feedback (looks like updating some instance property)\n",
    "            # print('calling from rl.incorporateFeedback(state, action, reward, newState)')\n",
    "            rl.incorporateFeedback(state, action, reward, newState)\n",
    "            totalReward += totalDiscount * reward\n",
    "            totalDiscount *= mdp.discount()\n",
    "            state = newState\n",
    "        if verbose:\n",
    "            print((\"Trial %d (totalReward = %s): %s\" %\n",
    "                   (trial, totalReward, sequence)))\n",
    "        totalRewards.append(totalReward)\n",
    "    return totalRewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_QL_over_MDP(mdp,featureExtractor):\n",
    "    # NOTE: adding more code to this function is totally optional, but it will probably be useful\n",
    "    # to you as you work to answer question 4b (a written question on this assignment).  We suggest\n",
    "    # that you add a few lines of code here to run value iteration, simulate Q-learning on the MDP,\n",
    "    # and then print some stats comparing the policies learned by these two approaches. Remember to\n",
    "    # set your explorationProb to zero after simulate.\n",
    "    # BEGIN_YOUR_CODE\n",
    "\n",
    "    rl = QLearningAlgorithm(mdp.actions, mdp.discount(),\n",
    "                            featureExtractor,\n",
    "                            0.2)\n",
    "    mdp.computeStates()\n",
    "\n",
    "    totalRewards = simulate(mdp, rl, numTrials=30000,\n",
    "                      maxIterations=1000, verbose=False, sort=False)\n",
    "\n",
    "    rl.explorationProb = 0\n",
    "\n",
    "    for state in mdp.states:\n",
    "        print(f'State: {state} ; Learned RL policy: {rl.getAction(state)}')\n",
    "\n",
    "    # print(len(mdp.states))\n",
    "    return totalRewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline():\n",
    "    def __init__(self, MDP_instance ,k):\n",
    "        # if just initiation, then we pick k boxes to be seen\n",
    "        self.MDP_instance = MDP_instance\n",
    "        self.space, self.box_tuple, _ = MDP_instance.startState()\n",
    "        box_list = list(self.box_tuple)\n",
    "        random.shuffle(box_list)\n",
    "        self.seen_boxes = box_list[:k]\n",
    "        self.unseen_boxes = box_list[k:]\n",
    "    \n",
    "#     @pysnooper.snoop()\n",
    "    def box_grab(self):\n",
    "\n",
    "        ## find widest boxed in the seen list\n",
    "        box_widest_width = max(self.seen_boxes, key = lambda x: x[1])[1]\n",
    "\n",
    "        seen_box_copy = deepcopy(self.seen_boxes)\n",
    "        \n",
    "        widest_boxes =[]\n",
    "\n",
    "        while max(seen_box_copy, key = lambda x: x[1])[1] == box_widest_width:\n",
    "\n",
    "            wide_index = seen_box_copy.index(max(seen_box_copy, key = lambda x: x[1]))\n",
    "\n",
    "            widest_boxes.append(seen_box_copy.pop(wide_index))\n",
    "\n",
    "            if len(seen_box_copy) == 0:\n",
    "                break\n",
    "\n",
    "        ## find tallest boxed in the widest box list\n",
    "        the_box = max(widest_boxes, key = lambda x: x[0])\n",
    "\n",
    "        # return tuple without the box\n",
    "        idx = self.seen_boxes.index(the_box)\n",
    "\n",
    "        self.seen_boxes = self.seen_boxes[:idx] + self.seen_boxes[idx+1:]\n",
    "        \n",
    "        #if still unseen box, random pick one more box from the unseen boxes\n",
    "        if len(self.unseen_boxes) > 0:\n",
    "\n",
    "            # random pick one more box from the unseen boxes\n",
    "            random.shuffle(self.unseen_boxes)\n",
    "\n",
    "            selected_box = self.unseen_boxes.pop(0)\n",
    "\n",
    "            self.seen_boxes.append(selected_box)\n",
    "        \n",
    "        return the_box\n",
    "    \n",
    "    \n",
    "    def find_position_to_drop_baseline(self, space, box):\n",
    "        '''\n",
    "        ignored many edge cases. \n",
    "        only suitable for baseline case (wide to narrow, tall to short)\n",
    "\n",
    "        '''\n",
    "        y_index = 0\n",
    "         #   if not suppassing the world's heigh\n",
    "        while y_index < len(space):\n",
    "            if y_index + box[0] > len(space):\n",
    "                return None\n",
    "\n",
    "            # check wether there is empty space on the y_axis\n",
    "            if min(space[y_index]) == 0:\n",
    "                #find where the empty space begins\n",
    "                empty_space_starting = space[y_index].index(min(space[y_index]))\n",
    "                # if space is enough\n",
    "                if len(space[0]) - (empty_space_starting ) >= box[1]:\n",
    "                    position = empty_space_starting\n",
    "                    return position\n",
    "            y_index += 1\n",
    "    \n",
    "#     @pysnooper.snoop()\n",
    "    def drop_box_base(self, space, box, position):\n",
    "\n",
    "        space = deepcopy(space)\n",
    "\n",
    "        # drop box\n",
    "        fall = False\n",
    "        index_y = 0\n",
    "        index_x = position\n",
    "\n",
    "        # if space has been occupied then we add index by 1\n",
    "        while max_2d(slice_tuple(index_y, index_y + box[0] , index_x, index_x+ box[1], space)  ) >0:\n",
    "            index_y = index_y +1\n",
    "\n",
    "        space = set_tuple(index_y,index_y + box[0] , index_x, index_x+ box[1] , space, 1)\n",
    "\n",
    "        ## fall if gravity center go out\n",
    "        ### get the y below the new box\n",
    "        if index_y >= 1:\n",
    "            #if not on ground, calculate the gravity center\n",
    "            below_index_y = index_y -1\n",
    "            center_of_gravity = index_x + (box[1])/2\n",
    "            # check whether two side of gravity center both have suport \n",
    "            if sum_2d( slice_tuple(below_index_y, below_index_y+1,  index_x , math.ceil(center_of_gravity), space) ) == 0 or \\\n",
    "                sum_2d( slice_tuple(below_index_y, below_index_y+1, math.ceil(center_of_gravity)-1, index_x + (box[1]) ,space) ) == 0:\n",
    "            # if fall set space as empty\n",
    "                fall = True\n",
    "\n",
    "        return space, fall\n",
    "\n",
    "\n",
    "    def run_baseline(self):\n",
    "        '''\n",
    "        run baseline strategy and return the rewards\n",
    "        '''\n",
    "        \n",
    "        ### see 5 box\n",
    "        box_number = len(self.box_tuple) \n",
    "        box_count = 1\n",
    "\n",
    "        while box_count <= box_number:\n",
    "#             print('seen_boxes:', self.seen_boxes)\n",
    "#             print('unseen_boxes',self.unseen_boxes)\n",
    "\n",
    "            the_box = self.box_grab()\n",
    "\n",
    "#             print(the_box)\n",
    "\n",
    "            drop_position = self.find_position_to_drop_baseline(self.space, the_box)\n",
    "\n",
    "            # if drop_position is string, that means we got\n",
    "            if drop_position is None:\n",
    "                raise BaseException('out of world bound')\n",
    "\n",
    "            self.space, fall = self.drop_box_base(self.space, the_box, drop_position)\n",
    "            \n",
    "        #        if fall, return 0 reward\n",
    "            if fall is True:\n",
    "                return -999, self.space\n",
    "\n",
    "            box_count += 1 \n",
    "\n",
    "#             print('seen_boxes:',self.seen_boxes)\n",
    "\n",
    "#             print('unseen_boxes:',self.unseen_boxes)\n",
    "\n",
    "#             print('**********************************************')\n",
    "\n",
    "        return self.MDP_instance.reward(self.space), self.space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = Baseline(box_t, 5)\n",
    "# while len(test.seen_boxes) > 0:\n",
    "#     print('seen ' , test.seen_boxes)\n",
    "#     print(test.box_grab())\n",
    "#     print('seen ' , test.seen_boxes)\n",
    "#     print('unseen ' , test.unseen_boxes)\n",
    "#     print('hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare baseline vs RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 states\n",
      "State: (((0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((2, 1), (2, 1)), (1, 2)) ; Learned RL policy: drop1\n",
      "State: (((0, 1, 0), (0, 1, 0), (0, 1, 1), (0, 0, 0), (0, 0, 0), (0, 0, 0)), (), (2, 1)) ; Learned RL policy: drop2\n",
      "State: (((1, 1, 1), (0, 1, 1), (0, 1, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), (), None) ; Learned RL policy: end\n",
      "State: (((1, 1, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((2, 1),), (2, 1)) ; Learned RL policy: drop2\n",
      "State: (None, ((2, 1),), None) ; Learned RL policy: pick0\n",
      "State: (((1, 0, 0), (1, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((1, 2), (2, 1)), None) ; Learned RL policy: pick1\n",
      "State: (((1, 1, 1), (1, 0, 1), (0, 0, 1), (0, 0, 0), (0, 0, 0), (0, 0, 0)), (), None) ; Learned RL policy: end\n",
      "State: (((0, 1, 1), (0, 0, 1), (0, 0, 1), (0, 0, 1), (0, 0, 1), (0, 0, 0)), (), None) ; Learned RL policy: end\n",
      "State: (((0, 1, 1), (0, 1, 0), (0, 1, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((2, 1),), None) ; Learned RL policy: pick0\n",
      "State: (((1, 1, 0), (1, 1, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), (), (1, 2)) ; Learned RL policy: drop0\n",
      "State: (((1, 1, 0), (1, 1, 0), (1, 1, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), (), None) ; Learned RL policy: end\n",
      "State: (((1, 1, 0), (0, 1, 0), (0, 1, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), (), (2, 1)) ; Learned RL policy: drop2\n",
      "State: (((1, 1, 0), (0, 1, 0), (0, 1, 0), (0, 1, 0), (0, 1, 0), (0, 0, 0)), (), None) ; Learned RL policy: end\n",
      "State: (((1, 1, 0), (1, 1, 0), (0, 1, 1), (0, 0, 0), (0, 0, 0), (0, 0, 0)), (), None) ; Learned RL policy: end\n",
      "State: (((1, 0, 0), (1, 0, 0), (1, 1, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((2, 1),), None) ; Learned RL policy: pick0\n",
      "State: (((0, 1, 1), (0, 1, 0), (0, 1, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), (), (2, 1)) ; Learned RL policy: drop2\n",
      "State: (((1, 1, 1), (1, 0, 1), (1, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), (), None) ; Learned RL policy: end\n",
      "State: (((0, 1, 1), (0, 1, 1), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((1, 2),), None) ; Learned RL policy: pick0\n",
      "State: (((0, 1, 1), (0, 1, 1), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), (), (1, 2)) ; Learned RL policy: drop1\n",
      "State: (((1, 1, 0), (1, 1, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((1, 2),), None) ; Learned RL policy: pick0\n",
      "State: (((1, 1, 0), (0, 1, 0), (0, 1, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((2, 1),), None) ; Learned RL policy: pick0\n",
      "State: (((0, 0, 1), (0, 0, 1), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((1, 2), (2, 1)), None) ; Learned RL policy: pick1\n",
      "State: (((0, 1, 0), (0, 1, 0), (0, 1, 0), (0, 1, 0), (0, 1, 1), (0, 0, 0)), (), None) ; Learned RL policy: end\n",
      "State: (((1, 0, 0), (1, 0, 0), (1, 1, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), (), (2, 1)) ; Learned RL policy: drop1\n",
      "State: (((1, 0, 1), (1, 0, 1), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), (), (1, 2)) ; Learned RL policy: drop0\n",
      "State: (((0, 1, 0), (0, 1, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((2, 1),), (1, 2)) ; Learned RL policy: drop1\n",
      "State: (((0, 1, 1), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((2, 1), (2, 1)), None) ; Learned RL policy: pick1\n",
      "State: (((1, 1, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((2, 1), (2, 1)), None) ; Learned RL policy: pick1\n",
      "State: (((1, 1, 1), (0, 0, 1), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), (), (2, 1)) ; Learned RL policy: drop2\n",
      "State: (((1, 0, 1), (1, 0, 1), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((1, 2),), None) ; Learned RL policy: pick0\n",
      "State: (((0, 1, 0), (0, 1, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((1, 2),), (2, 1)) ; Learned RL policy: drop2\n",
      "State: (((0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((1, 2), (2, 1), (2, 1)), None) ; Learned RL policy: pick2\n",
      "State: (((1, 1, 1), (0, 0, 1), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((2, 1),), None) ; Learned RL policy: pick0\n",
      "State: (((0, 1, 1), (0, 0, 1), (0, 0, 1), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((2, 1),), None) ; Learned RL policy: pick0\n",
      "State: (((0, 1, 1), (0, 1, 0), (0, 1, 0), (0, 1, 0), (0, 1, 0), (0, 0, 0)), (), None) ; Learned RL policy: end\n",
      "State: (((0, 1, 0), (0, 1, 0), (0, 1, 0), (0, 1, 0), (0, 0, 0), (0, 0, 0)), (), (1, 2)) ; Learned RL policy: drop1\n",
      "State: (((1, 1, 1), (1, 1, 0), (0, 1, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), (), None) ; Learned RL policy: end\n",
      "State: (((1, 0, 0), (1, 0, 0), (1, 0, 0), (1, 0, 0), (0, 0, 0), (0, 0, 0)), (), (1, 2)) ; Learned RL policy: drop1\n",
      "State: (((0, 0, 1), (0, 0, 1), (0, 0, 1), (0, 0, 1), (0, 0, 0), (0, 0, 0)), (), (1, 2)) ; Learned RL policy: drop0\n",
      "State: (((0, 1, 1), (0, 0, 1), (0, 0, 1), (0, 0, 0), (0, 0, 0), (0, 0, 0)), (), (2, 1)) ; Learned RL policy: drop2\n",
      "State: (((1, 1, 1), (0, 0, 1), (0, 0, 1), (0, 0, 1), (0, 0, 0), (0, 0, 0)), (), None) ; Learned RL policy: end\n",
      "State: (((1, 0, 0), (1, 0, 0), (1, 0, 0), (1, 0, 0), (1, 1, 0), (0, 0, 0)), (), None) ; Learned RL policy: end\n",
      "State: (((1, 0, 0), (1, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((2, 1),), (1, 2)) ; Learned RL policy: drop1\n",
      "State: (((1, 1, 1), (1, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((2, 1),), None) ; Learned RL policy: pick0\n",
      "State: (None, (), None) ; Learned RL policy: end\n",
      "State: (((0, 1, 0), (0, 1, 0), (0, 1, 0), (0, 1, 0), (0, 0, 0), (0, 0, 0)), ((1, 2),), None) ; Learned RL policy: pick0\n",
      "State: (((0, 0, 1), (0, 0, 1), (0, 0, 1), (0, 0, 1), (0, 0, 0), (0, 0, 0)), ((1, 2),), None) ; Learned RL policy: pick0\n",
      "State: (((1, 0, 1), (1, 0, 1), (1, 1, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), (), None) ; Learned RL policy: end\n",
      "State: (((1, 0, 0), (1, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((1, 2),), (2, 1)) ; Learned RL policy: drop1\n",
      "State: (((1, 1, 1), (1, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), (), (2, 1)) ; Learned RL policy: drop1\n",
      "State: (((1, 1, 1), (1, 0, 0), (1, 0, 0), (1, 0, 0), (0, 0, 0), (0, 0, 0)), (), None) ; Learned RL policy: end\n",
      "State: (((1, 1, 0), (1, 0, 0), (1, 0, 0), (1, 0, 0), (1, 0, 0), (0, 0, 0)), (), None) ; Learned RL policy: end\n",
      "State: (((0, 1, 0), (0, 1, 0), (0, 1, 1), (0, 1, 0), (0, 1, 0), (0, 0, 0)), (), None) ; Learned RL policy: end\n",
      "State: (((1, 0, 0), (1, 0, 0), (1, 1, 0), (1, 0, 0), (1, 0, 0), (0, 0, 0)), (), None) ; Learned RL policy: end\n",
      "State: (((0, 1, 1), (0, 1, 1), (0, 1, 1), (0, 0, 0), (0, 0, 0), (0, 0, 0)), (), None) ; Learned RL policy: end\n",
      "State: (((0, 0, 1), (0, 0, 1), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((2, 1),), (1, 2)) ; Learned RL policy: drop0\n",
      "State: (((1, 0, 0), (1, 0, 0), (1, 0, 0), (1, 0, 0), (0, 0, 0), (0, 0, 0)), ((1, 2),), None) ; Learned RL policy: pick0\n",
      "State: (((0, 1, 1), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((2, 1),), (2, 1)) ; Learned RL policy: drop2\n",
      "State: (((0, 1, 0), (0, 1, 0), (0, 1, 1), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((2, 1),), None) ; Learned RL policy: pick0\n",
      "State: (((1, 1, 0), (1, 0, 0), (1, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), (), (2, 1)) ; Learned RL policy: drop1\n",
      "State: (((0, 1, 0), (0, 1, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((1, 2), (2, 1)), None) ; Learned RL policy: pick1\n",
      "State: (((0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((1, 2), (2, 1)), (2, 1)) ; Learned RL policy: drop2\n",
      "State: (((0, 0, 1), (0, 0, 1), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((1, 2),), (2, 1)) ; Learned RL policy: drop2\n",
      "State: (((1, 1, 0), (1, 0, 0), (1, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)), ((2, 1),), None) ; Learned RL policy: pick0\n"
     ]
    }
   ],
   "source": [
    "# randomMDP = BoxMDP(generate_n_random_box(3, 3), 5, 9)\n",
    "fixMDP = BoxMDP(((1, 2), (2, 1), (2, 1)), 3, 6)\n",
    "# print(randomMDP.startState()    )\n",
    "_ = simulate_QL_over_MDP(fixMDP, identityFeatureExtractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_random_box(n, biggest_size):\n",
    "    \n",
    "    def random_box(biggest_size):\n",
    "        return (random.choice(list(range(1, biggest_size+1))), random.choice(list(range(1, biggest_size+1))))\n",
    "\n",
    "    random_boxes = []\n",
    "    for i in range(n):\n",
    "        random_boxes.append(random_box(biggest_size))\n",
    "    return random_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "box_numbers = [2, 3]\n",
    "world_sizes = [3, 4]\n",
    "\n",
    "reward_dict = collections.defaultdict(list)\n",
    "\n",
    "for i in itertools.product(box_numbers, world_sizes):\n",
    "    box_number = i[0]\n",
    "    world_size = i[1]\n",
    "    print(f'box number {box_number}, world size {world_size}')\n",
    "    \n",
    "    run_time = 1\n",
    "    \n",
    "    baseline_rewards = []\n",
    "    rl_rewards = []\n",
    "\n",
    "    while run_time <= 20:\n",
    "        #                                        # size width height\n",
    "        randomMDP = BoxMDP(generate_n_random_box(box_number, 3), world_size, 3*box_number)\n",
    "\n",
    "        baseline_instance = Baseline(randomMDP, 3)\n",
    "        b_reward, _ = baseline_instance.run_baseline()\n",
    "\n",
    "        baseline_rewards.append(b_reward)\n",
    "\n",
    "#         print('finish baseline')\n",
    "\n",
    "        r_reward = simulate_QL_over_MDP(randomMDP, identityFeatureExtractor)\n",
    "\n",
    "        mean_reward = round(sum(r_reward)/len(r_reward), 1)\n",
    "\n",
    "        rl_rewards.append(mean_reward)\n",
    "\n",
    "#         print('finish rl')\n",
    "\n",
    "        run_time += 1\n",
    "    \n",
    "    for x, y in zip(baseline_rewards, rl_rewards):\n",
    "#         print(f'reward comparison: baseline: {x} RL: {y}') \n",
    "        reward_dict[f'baseline_n_{box_number}_w_{world_size}'].append(x)\n",
    "        reward_dict[f'RL_n_{box_number}_w_{world_size}'].append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 runs, baseline average: -0.45, RL average -0.13999999999999999\n"
     ]
    }
   ],
   "source": [
    "# print(f'{run_time-1} runs, baseline average: {np.mean(baseline_rewards)}, RL average {np.mean(rl_rewards)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_csv = pd.DataFrame(reward_dict)\n",
    "raw_csv.to_csv('raw_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_csv = pd.DataFrame(raw_csv.apply(lambda x: round(np.mean(x),4), axis = 0), columns=['mean'])\n",
    "mean_csv.to_csv('mean_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
